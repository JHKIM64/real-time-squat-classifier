{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMR0/msI1cc2oabdvezemHS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omin-Kwon/real-time-squat-classifier/blob/main/model/SquatClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fXF_T3bSEa_"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqEDSdeoOXpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ce67f1-852c-453d-f61b-8833ea71900b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c57lCStiOyqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc9f11d-687c-458b-f49b-c6eac603cbb2"
      },
      "source": [
        "\"\"\"\n",
        "Change directory to where this file is located\n",
        "\"\"\"\n",
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_qq-N3MSOZc"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import ZeroPadding1D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "from scipy import special\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "71pC0E6Rh_Us"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gloabal parameters"
      ],
      "metadata": {
        "id": "PUJN9gg__eYm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTMPV-Aa4DwS"
      },
      "source": [
        "data_path = './Pose_Dataset/'\n",
        "\n",
        "image_size = 128\n",
        "min_dim = 100\n",
        "max_obj_num = 100\n",
        "\n",
        "unet_base_filters = 64\n",
        "in_channels = 3\n",
        "\n",
        "w0 = 10\n",
        "sigma = 5\n",
        "\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "labels=['bad_back_round','bad_back_warp','bad_head','bad_innner_thigh','bad_shallow','bad_toe','good']\n",
        "paths={'bad_back_round':0,\n",
        "       'bad_back_warp':0,\n",
        "       'bad_head':0,\n",
        "       'bad_innner_thigh':0,\n",
        "       'bad_shallow':0,\n",
        "       'bad_toe':0,\n",
        "       'good':0}\n",
        "\n",
        "for label in labels:\n",
        "    paths[label]=data_path+label+\"/1115_3djoints_index\"\n",
        "\n",
        "\n",
        "json_files={'bad_back_round':0,\n",
        "       'bad_back_warp':0,\n",
        "       'bad_head':0,\n",
        "       'bad_innner_thigh':0,\n",
        "       'bad_shallow':0,\n",
        "       'bad_toe':0,\n",
        "       'good':0}\n",
        "\n",
        "\n",
        "for label in labels:\n",
        "    json_files[label]=[pos_json for pos_json in os.listdir(paths[label]) if pos_json.endswith('.json')]\n",
        "\n",
        "data={'bad_back_round':[],'bad_back_warp':[],'bad_head':[],'bad_innner_thigh':[],'bad_shallow':[],'bad_toe':[],'good':[]}\n",
        "\n",
        "for label in labels:\n",
        "    for single_file in json_files[label]:\n",
        "        file_path=paths[label]+\"/\"+single_file\n",
        "        with open(file_path,'r') as fp:\n",
        "            data[label].append(json.load(fp))\n",
        "\n"
      ],
      "metadata": {
        "id": "m9EfzcObkbjq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_numpy={}\n",
        "for label in labels:\n",
        "    label_list=[]\n",
        "    for i in range(len(data[label])):\n",
        "        my_list=[]\n",
        "        for j in range(300):\n",
        "            my_list.append(data[label][i][str(j)]['3d_joint'])\n",
        "        label_list.append(my_list)\n",
        "    label_numpy=np.array(label_list)\n",
        "    data_numpy[label]=label_numpy"
      ],
      "metadata": {
        "id": "9bhjqVcxMyv5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples={}\n",
        "valid_samples={}\n",
        "test_samples={}\n",
        "labels=['bad_back_round','bad_back_warp','bad_head','bad_innner_thigh','bad_shallow','bad_toe','good']\n",
        "label_encoding={'bad_back_round':[1,0,0,0,0,0,0],'bad_back_warp':[0,1,0,0,0,0,0],'bad_head':[0,0,1,0,0,0,0],'bad_innner_thigh':[0,0,0,1,0,0,0],'bad_shallow':[0,0,0,0,1,0,0],'bad_toe':[0,0,0,0,0,1,0],'good':[0,0,0,0,0,0,1]}\n",
        "train_labels=[]\n",
        "valid_labels=[]\n",
        "test_labels=[]\n",
        "train_x=[]\n",
        "valid_x=[]\n",
        "test_x=[]\n",
        "\n",
        "#train: valid: test = 5:3:2로 나눠봄\n",
        "for label in labels:\n",
        "    np.random.shuffle(data_numpy[label])\n",
        "    num_train=int(0.5*len(data_numpy[label]))\n",
        "    num_valid=int(0.8*len(data_numpy[label]))\n",
        "    train_samples[label],valid_samples[label],test_samples[label]=data_numpy[label][:num_train,:],data_numpy[label][num_train:num_valid,:],data_numpy[label][num_valid:,:]\n",
        "\n",
        "for label in labels:\n",
        "    #print(\"----\"+label+\"-----\")\n",
        "    for i in range(train_samples[label].shape[0]):\n",
        "        train_labels.append(label_encoding[label])\n",
        "        train_x.append(train_samples[label][i,:,:].transpose())\n",
        "    \n",
        "    for i in range(valid_samples[label].shape[0]):\n",
        "        valid_labels.append(label_encoding[label])\n",
        "        valid_x.append(valid_samples[label][i,:,:].transpose())\n",
        "    \n",
        "    for i in range(test_samples[label].shape[0]):\n",
        "        test_labels.append(label_encoding[label])   \n",
        "        test_x.append(train_samples[label][i,:,:].transpose()) \n",
        "\n",
        "train_x=np.array(train_x)\n",
        "train_labels=np.array(train_labels)\n",
        "\n",
        "valid_x=np.array(valid_x)\n",
        "valid_labels=np.array(valid_labels)\n",
        "test_x=np.array(test_x)\n",
        "test_labels=np.array(test_labels)\n",
        "\n",
        "print(\"train_x\",train_x.shape)\n",
        "print(\"train_label\",train_labels.shape)\n",
        "\n",
        "print(\"test_x\",test_x.shape)\n",
        "print(\"test_label\",test_labels.shape)\n",
        "\n",
        "print(\"valid_x\",valid_x.shape)\n",
        "print(\"valid_label\",valid_labels.shape)\n",
        "\n",
        "train_x,train_labels=shuffle(train_x,train_labels)\n",
        "valid_x,valid_labels=shuffle(valid_x,valid_labels)\n",
        "test_x,test_labels=shuffle(test_x,test_labels)\n"
      ],
      "metadata": {
        "id": "Nf8xrDHVUcpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134057b7-989c-4435-ea6a-6da33b7127f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_x (1000, 171, 300)\n",
            "train_label (1000, 7)\n",
            "test_x (403, 171, 300)\n",
            "test_label (403, 7)\n",
            "valid_x (601, 171, 300)\n",
            "valid_label (601, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define ResnetBlock"
      ],
      "metadata": {
        "id": "4Do_iXhyLm-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resblock(frames):\n",
        "    #resblock에 들어온 input_layer\n",
        "    input_layer=Input(shape=frames)\n",
        "    \n",
        "    res_layer=Conv1D(filters=256,kernel_size=1,strides=2)(input_layer)\n",
        "    \n",
        "    conv1d_layer1=Conv1D(filters=256,kernel_size=3,strides=2)(input_layer)\n",
        "    conv1d_layer1=tf.keras.layers.ZeroPadding1D(padding=(0, 1))(conv1d_layer1)\n",
        "    batch1=BatchNormalization()(conv1d_layer1)\n",
        "    active1=activations.relu(batch1)\n",
        "\n",
        "    conv1d_layer2=Conv1D(filters=256,kernel_size=3,strides=1,padding='same')(active1)\n",
        "    batch2=BatchNormalization()(conv1d_layer2)\n",
        "    active2=activations.relu(batch2)\n",
        "\n",
        "\n",
        "    conv1d_layer3=Conv1D(filters=256,kernel_size=3,strides=1,padding='same')(active2)\n",
        "    batch3=BatchNormalization()(conv1d_layer3)\n",
        "    active3=activations.relu(batch3)\n",
        "   \n",
        "    output=tf.keras.layers.Add()([res_layer,active3])\n",
        "    block=tf.keras.Model(inputs=input_layer,outputs=output)\n",
        "    \n",
        "    return block\n",
        "    \n"
      ],
      "metadata": {
        "id": "WBzAXpodOrwy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model"
      ],
      "metadata": {
        "id": "_YZrQMIzV448"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "points=[]\n",
        "outputs=[]\n",
        "\n",
        "\n",
        "inputs=tf.keras.Input(shape=(171,300,1))\n",
        "\n",
        "# (B,171,300,1) matrix를 (B.300,1) matrix 171 개로 찢어준다.\n",
        "for i in range(171):\n",
        "    points.append(inputs[:,i,:,:])\n",
        "\n",
        "\n",
        "for i in range(171):\n",
        "    points[i]=Conv1D(filters=256,kernel_size=3,strides=2)(points[i])\n",
        "    points[i]=ZeroPadding1D(padding=(0, 1))(points[i])\n",
        "    points[i]=resblock((150,256))(points[i])\n",
        "    points[i]=resblock((75,256))(points[i])\n",
        "    points[i]=resblock((38,256))(points[i])\n",
        "    points[i]=resblock((19,256))(points[i])\n",
        "\n",
        "for i in range(171):\n",
        "    points[i]=tf.expand_dims(points[i],axis=1)\n",
        "    #print(points[i].shape)\n",
        "\n",
        "point2D=tf.concat(points,axis=1)\n",
        "#print(point2D.shape)\n",
        "\n",
        "\n",
        "avg_pool_2d = AveragePooling2D(pool_size=(171, 10),  strides=(1, 1), padding='valid')\n",
        "\n",
        "point2D=avg_pool_2d(point2D)\n",
        "#print(point2D.shape)\n",
        "point2D=Conv1D(filters=7,kernel_size=1,strides=1,activation='softmax')(point2D)\n",
        "#print(point2D.shape)\n",
        "\n",
        "point2D=point2D[:,0,0,:]\n",
        "#print(point2D.shape)\n",
        "\n",
        "my_model=tf.keras.Model(inputs=inputs,outputs=point2D)"
      ],
      "metadata": {
        "id": "K8RlqYafGq41"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "my_model.fit(x=train_x,y=train_labels,batch_size=10,epochs=30,validation_data=(valid_x,valid_labels))\n",
        "\n",
        "\n",
        "\"\"\"Model.fit(\n",
        "    x=None,\n",
        "    y=None,\n",
        "    batch_size=None,\n",
        "    epochs=1,\n",
        "    verbose=\"auto\",\n",
        "    callbacks=None,\n",
        "    validation_split=0.0,\n",
        "    validation_data=None,\n",
        "    shuffle=True,\n",
        "    class_weight=None,\n",
        "    sample_weight=None,\n",
        "    initial_epoch=0,\n",
        "    steps_per_epoch=None,\n",
        "    validation_steps=None,\n",
        "    validation_batch_size=None,\n",
        "    validation_freq=1,\n",
        "    max_queue_size=10,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False,\n",
        ")\"\"\""
      ],
      "metadata": {
        "id": "Epb1lCBYNLRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e39b9f0-8d65-4f9f-ad6a-090295095ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        }
      ]
    }
  ]
}